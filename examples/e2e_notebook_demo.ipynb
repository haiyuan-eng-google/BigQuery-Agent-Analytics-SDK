{
 "cells": [
   {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright-header"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notebook-affordances"
      },
      "source": [
        "# Demo Plan: BigQuery for Agent Ops - Unified Platform\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/haiyuan-eng-google/demo_BQ_agent_analytics_plugin_notebook/blob/main/Demo_Plan_BigQuery_for_Agent_Ops_Unified_Platform_Public.ipynb\">\n",
        "      <img src=\"https://raw.githubusercontent.com/googleapis/python-bigquery-dataframes/refs/heads/main/third_party/logo/colab-logo.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/haiyuan-eng-google/demo_BQ_agent_analytics_plugin_notebook/blob/main/Demo_Plan_BigQuery_for_Agent_Ops_Unified_Platform_Public.ipynb\">\n",
        "      <img src=\"https://raw.githubusercontent.com/googleapis/python-bigquery-dataframes/refs/heads/main/third_party/logo/github-logo.png\" width=\"32\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/haiyuan-eng-google/demo_BQ_agent_analytics_plugin_notebook/main/Demo_Plan_BigQuery_for_Agent_Ops_Unified_Platform_Public.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/product/1x/google_cloud_48dp.png\" alt=\"Vertex AI logo\" width=\"32\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/bigquery/import?url=https://github.com/haiyuan-eng-google/demo_BQ_agent_analytics_plugin_notebook/blob/main/Demo_Plan_BigQuery_for_Agent_Ops_Unified_Platform_Public.ipynb\">\n",
        "      <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTW1gvOovVlbZAIZylUtf5Iu8-693qS1w5NJw&s\" alt=\"BQ logo\" width=\"35\">\n",
        "      Open in BQ Studio\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-adk bigquery-agent-analytics google-cloud-bigquery nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate & Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Colab authentication\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    print(\"Colab authentication successful.\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab — using default credentials.\")\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"test-project-0728-467323\")\n",
    "DATASET_ID = os.environ.get(\"BQ_DATASET\", \"agent_analytics\")\n",
    "TABLE_ID = os.environ.get(\"BQ_TABLE\", \"agent_events_v2\")\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"gemini-3-flash-preview\")\n",
    "LOCATION = \"US\"\n",
    "APP_NAME = \"e2e_notebook_demo\"\n",
    "USER_ID = \"demo_user\"\n",
    "\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"global\"\n",
    "\n",
    "# Enable async in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(f\"Project  : {PROJECT_ID}\")\n",
    "print(f\"Dataset  : {DATASET_ID}\")\n",
    "print(f\"Table    : {TABLE_ID}\")\n",
    "print(f\"Model    : {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1: Run Agent & Log Traces to BigQuery\n",
    "\n",
    "We define a **travel planner agent** with four deterministic tools, run three conversations, and log every event to BigQuery via the `BigQueryAgentAnalyticsPlugin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "async def search_flights(\n",
    "    origin: str,\n",
    "    destination: str,\n",
    "    date: str,\n",
    "    max_results: int = 5,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Search for available flights between two cities.\n",
    "\n",
    "    Args:\n",
    "        origin: Departure city or airport code.\n",
    "        destination: Arrival city or airport code.\n",
    "        date: Travel date in YYYY-MM-DD format.\n",
    "        max_results: Maximum number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with flight search results.\n",
    "    \"\"\"\n",
    "    seed = int(\n",
    "        hashlib.md5(f\"{origin}{destination}{date}\".encode()).hexdigest()[:8],\n",
    "        16,\n",
    "    )\n",
    "    rng = random.Random(seed)\n",
    "    airlines = [\n",
    "        \"United Airlines\", \"Delta Air Lines\", \"American Airlines\",\n",
    "        \"JetBlue Airways\", \"Southwest Airlines\", \"Alaska Airlines\",\n",
    "    ]\n",
    "    flights = []\n",
    "    for i in range(min(max_results, 5)):\n",
    "        dep_hour = rng.randint(6, 20)\n",
    "        duration_h = rng.randint(2, 14)\n",
    "        flights.append({\n",
    "            \"flight_id\": f\"FL-{seed + i:06d}\",\n",
    "            \"airline\": rng.choice(airlines),\n",
    "            \"origin\": origin,\n",
    "            \"destination\": destination,\n",
    "            \"date\": date,\n",
    "            \"departure_time\": f\"{dep_hour:02d}:{rng.choice(['00','15','30','45'])}\",\n",
    "            \"arrival_time\": f\"{(dep_hour + duration_h) % 24:02d}:{rng.choice(['00','15','30','45'])}\",\n",
    "            \"duration_hours\": duration_h,\n",
    "            \"price_usd\": round(rng.uniform(150, 1200), 2),\n",
    "            \"class\": rng.choice([\"Economy\", \"Premium Economy\", \"Business\"]),\n",
    "            \"stops\": rng.choice([0, 0, 0, 1, 1, 2]),\n",
    "        })\n",
    "    return {\n",
    "        \"query\": {\"origin\": origin, \"destination\": destination, \"date\": date},\n",
    "        \"results_count\": len(flights),\n",
    "        \"flights\": flights,\n",
    "    }\n",
    "\n",
    "\n",
    "async def search_hotels(\n",
    "    city: str,\n",
    "    check_in: str,\n",
    "    check_out: str,\n",
    "    max_results: int = 5,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Search for hotels in a given city.\n",
    "\n",
    "    Args:\n",
    "        city: City name to search hotels in.\n",
    "        check_in: Check-in date (YYYY-MM-DD).\n",
    "        check_out: Check-out date (YYYY-MM-DD).\n",
    "        max_results: Maximum number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with hotel search results.\n",
    "    \"\"\"\n",
    "    seed = int(hashlib.md5(f\"{city}{check_in}\".encode()).hexdigest()[:8], 16)\n",
    "    rng = random.Random(seed)\n",
    "    hotel_names = [\n",
    "        f\"Grand {city} Hotel\", f\"{city} Plaza\",\n",
    "        f\"The {city} Marriott\", f\"Hilton {city} Downtown\",\n",
    "        f\"Hyatt Regency {city}\", f\"Four Seasons {city}\",\n",
    "        f\"Holiday Inn {city}\",\n",
    "    ]\n",
    "    hotels = []\n",
    "    for i in range(min(max_results, 5)):\n",
    "        rating = round(rng.uniform(3.5, 5.0), 1)\n",
    "        hotels.append({\n",
    "            \"hotel_id\": f\"HT-{seed + i:06d}\",\n",
    "            \"name\": hotel_names[i % len(hotel_names)],\n",
    "            \"city\": city,\n",
    "            \"check_in\": check_in,\n",
    "            \"check_out\": check_out,\n",
    "            \"rating\": rating,\n",
    "            \"price_per_night_usd\": round(rng.uniform(80, 500), 2),\n",
    "            \"amenities\": rng.sample(\n",
    "                [\"WiFi\", \"Pool\", \"Gym\", \"Spa\", \"Restaurant\",\n",
    "                 \"Bar\", \"Room Service\", \"Parking\",\n",
    "                 \"Airport Shuttle\", \"Business Center\"],\n",
    "                k=rng.randint(3, 7),\n",
    "            ),\n",
    "            \"distance_to_center_km\": round(rng.uniform(0.2, 8.0), 1),\n",
    "        })\n",
    "    return {\n",
    "        \"query\": {\"city\": city, \"check_in\": check_in, \"check_out\": check_out},\n",
    "        \"results_count\": len(hotels),\n",
    "        \"hotels\": hotels,\n",
    "    }\n",
    "\n",
    "\n",
    "async def get_weather_forecast(\n",
    "    city: str,\n",
    "    date: str,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Get weather forecast for a city on a specific date.\n",
    "\n",
    "    Args:\n",
    "        city: City name.\n",
    "        date: Date in YYYY-MM-DD format.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with weather forecast data.\n",
    "    \"\"\"\n",
    "    seed = int(hashlib.md5(f\"{city}{date}\".encode()).hexdigest()[:8], 16)\n",
    "    rng = random.Random(seed)\n",
    "    conditions = [\n",
    "        \"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Light Rain\",\n",
    "        \"Rain\", \"Thunderstorms\", \"Clear\", \"Overcast\",\n",
    "    ]\n",
    "    return {\n",
    "        \"city\": city,\n",
    "        \"date\": date,\n",
    "        \"temperature_high_c\": rng.randint(15, 35),\n",
    "        \"temperature_low_c\": rng.randint(5, 20),\n",
    "        \"condition\": rng.choice(conditions),\n",
    "        \"humidity_pct\": rng.randint(30, 90),\n",
    "        \"wind_speed_kmh\": rng.randint(5, 40),\n",
    "        \"precipitation_chance_pct\": rng.randint(0, 80),\n",
    "        \"uv_index\": rng.randint(1, 11),\n",
    "    }\n",
    "\n",
    "\n",
    "async def calculate_trip_budget(\n",
    "    flights: float,\n",
    "    hotels: float,\n",
    "    daily_expenses: float,\n",
    "    num_days: int,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Calculate total trip budget from component costs.\n",
    "\n",
    "    Args:\n",
    "        flights: Total flight cost in USD.\n",
    "        hotels: Total hotel cost in USD.\n",
    "        daily_expenses: Estimated daily expenses (food, transport, etc.).\n",
    "        num_days: Number of trip days.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with itemised budget breakdown.\n",
    "    \"\"\"\n",
    "    total_daily = daily_expenses * num_days\n",
    "    subtotal = flights + hotels + total_daily\n",
    "    tax_and_fees = round(subtotal * 0.12, 2)\n",
    "    total = round(subtotal + tax_and_fees, 2)\n",
    "    return {\n",
    "        \"breakdown\": {\n",
    "            \"flights\": round(flights, 2),\n",
    "            \"hotels\": round(hotels, 2),\n",
    "            \"daily_expenses_total\": round(total_daily, 2),\n",
    "            \"daily_expenses_per_day\": round(daily_expenses, 2),\n",
    "            \"num_days\": num_days,\n",
    "            \"tax_and_fees\": tax_and_fees,\n",
    "        },\n",
    "        \"subtotal_usd\": round(subtotal, 2),\n",
    "        \"total_usd\": total,\n",
    "        \"currency\": \"USD\",\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Tool functions defined: search_flights, search_hotels, get_weather_forecast, calculate_trip_budget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.genai import types\n",
    "\n",
    "TRAVEL_PLANNER_INSTRUCTION = \"\"\"\\\n",
    "You are a helpful travel planning assistant. You help users plan trips by\n",
    "searching for flights, hotels, checking weather forecasts, and calculating\n",
    "budgets.\n",
    "\n",
    "Guidelines:\n",
    "- Always search for flights and hotels when the user asks to plan a trip.\n",
    "- Check the weather at the destination when relevant.\n",
    "- Provide a budget estimate when enough cost information is available.\n",
    "- Be concise but informative in your responses.\n",
    "- Present results in a clear, organized format.\n",
    "- When multiple tools are needed, call them as appropriate and then\n",
    "  synthesize the results into a cohesive plan.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_agent() -> LlmAgent:\n",
    "    \"\"\"Build the travel planner agent.\"\"\"\n",
    "    return LlmAgent(\n",
    "        name=\"travel_planner\",\n",
    "        model=MODEL_NAME,\n",
    "        instruction=TRAVEL_PLANNER_INSTRUCTION,\n",
    "        tools=[\n",
    "            search_flights,\n",
    "            search_hotels,\n",
    "            get_weather_forecast,\n",
    "            calculate_trip_budget,\n",
    "        ],\n",
    "        generate_content_config=types.GenerateContentConfig(\n",
    "            temperature=1.0,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Agent builder ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import uuid\n",
    "\n",
    "from google.adk.plugins.bigquery_agent_analytics_plugin import (\n",
    "    BigQueryAgentAnalyticsPlugin,\n",
    "    BigQueryLoggerConfig,\n",
    ")\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "\n",
    "# Build agent, runner, and plugin\n",
    "agent = build_agent()\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "plugin = BigQueryAgentAnalyticsPlugin(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    config=BigQueryLoggerConfig(\n",
    "        table_id=TABLE_ID,\n",
    "        batch_size=1,\n",
    "        batch_flush_interval=1.0,\n",
    "    ),\n",
    "    location=LOCATION,\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    app_name=APP_NAME,\n",
    "    session_service=session_service,\n",
    "    plugins=[plugin],\n",
    ")\n",
    "\n",
    "# Define three conversations\n",
    "conversations = [\n",
    "    {\n",
    "        \"label\": \"Simple trip (SF -> NY)\",\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"Plan a weekend trip from San Francisco to New York\"\n",
    "                \" departing 2025-04-12 and returning 2025-04-14.\"\n",
    "                \" Search flights for April 12 and hotels checking\"\n",
    "                \" in April 12, checking out April 14.\"\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Complex trip (LA -> Tokyo)\",\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"I want to plan a 5-day vacation to Tokyo from\"\n",
    "                \" 2025-05-01 to 2025-05-06. Search flights from\"\n",
    "                \" Los Angeles departing 2025-05-01, find hotels in\"\n",
    "                \" Tokyo checking in 2025-05-01 and checking out\"\n",
    "                \" 2025-05-06, check the weather for 2025-05-02,\"\n",
    "                \" and calculate the budget with the flight and\"\n",
    "                \" hotel prices you find plus $150/day expenses\"\n",
    "                \" for 5 days.\"\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Multi-turn (Chicago -> Paris)\",\n",
    "        \"messages\": [\n",
    "            \"What's the weather like in Paris on 2025-04-20?\",\n",
    "            \"Find me flights from Chicago to Paris on 2025-04-20.\",\n",
    "            (\n",
    "                \"Now find hotels in Paris checking in 2025-04-20\"\n",
    "                \" and checking out 2025-04-25.\"\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "async def run_conversation(messages, label=\"\"):\n",
    "    \"\"\"Run a multi-turn conversation and return the session_id.\"\"\"\n",
    "    session_id = f\"e2e-{uuid.uuid4().hex[:12]}\"\n",
    "    await session_service.create_session(\n",
    "        app_name=APP_NAME,\n",
    "        user_id=USER_ID,\n",
    "        session_id=session_id,\n",
    "    )\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  Session: {session_id}  [{label}]\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    for i, message in enumerate(messages, 1):\n",
    "        print(f\"\\n[Turn {i}] User: {message}\")\n",
    "        print(\"-\" * 48)\n",
    "        user_content = types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=message)],\n",
    "        )\n",
    "        response_parts = []\n",
    "        async for event in runner.run_async(\n",
    "            user_id=USER_ID,\n",
    "            session_id=session_id,\n",
    "            new_message=user_content,\n",
    "        ):\n",
    "            if event.content and event.content.parts:\n",
    "                for part in event.content.parts:\n",
    "                    if hasattr(part, \"text\") and part.text:\n",
    "                        response_parts.append(part.text)\n",
    "                    elif hasattr(part, \"function_call\") and part.function_call:\n",
    "                        print(f\"  -> Tool call: {part.function_call.name}\")\n",
    "        if response_parts:\n",
    "            text = \"\\n\".join(response_parts)\n",
    "            print(f\"\\n[Agent]: {text[:1000]}\")\n",
    "            if len(text) > 1000:\n",
    "                print(f\"  ... (truncated, {len(text)} chars total)\")\n",
    "    return session_id\n",
    "\n",
    "\n",
    "# Run all conversations and collect session IDs\n",
    "session_ids = []\n",
    "for conv in conversations:\n",
    "    sid = asyncio.get_event_loop().run_until_complete(\n",
    "        run_conversation(conv[\"messages\"], label=conv[\"label\"])\n",
    "    )\n",
    "    session_ids.append(sid)\n",
    "\n",
    "print(f\"\\n\\nSession IDs: {session_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Flush plugin to ensure all events are written\n",
    "print(\"Flushing traces to BigQuery ...\")\n",
    "try:\n",
    "    asyncio.get_event_loop().run_until_complete(plugin.flush())\n",
    "except Exception as exc:\n",
    "    print(f\"Flush warning: {exc}\")\n",
    "\n",
    "settle_seconds = 15\n",
    "print(f\"Waiting {settle_seconds}s for BigQuery data to settle ...\")\n",
    "time.sleep(settle_seconds)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2: Trace Retrieval & Visualization\n",
    "\n",
    "Now that traces are in BigQuery, we use the **SDK Client** to fetch them. Each `Trace` contains a hierarchical span tree that can be rendered as a DAG. We can also inspect tool calls, the final response, and any error spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import Client, TraceFilter\n",
    "\n",
    "client = Client(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    table_id=TABLE_ID,\n",
    "    location=LOCATION,\n",
    "    endpoint=MODEL_NAME,\n",
    ")\n",
    "print(\"SDK Client initialised.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Retrieve and render each trace\ntraces = []\nfor sid in session_ids:\n    try:\n        trace = client.get_trace(sid)\n        traces.append(trace)\n        print(f\"\\n{'=' * 60}\")\n        print(f\"  Trace for session: {sid}\")\n        print(f\"{'=' * 60}\")\n        _ = trace.render()  # render() prints and returns the tree\n    except Exception as exc:\n        print(f\"Could not retrieve trace {sid}: {exc}\")\n        traces.append(None)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect trace properties\n",
    "for i, trace in enumerate(traces):\n",
    "    if trace is None:\n",
    "        continue\n",
    "    print(f\"\\n--- Session {i+1}: {trace.session_id} ---\")\n",
    "    print(f\"  Tool calls: {len(trace.tool_calls)}\")\n",
    "    for tc in trace.tool_calls:\n",
    "        print(f\"    - {tc.get('tool_name', '?')}\")\n",
    "    final = trace.final_response or \"(none)\"\n",
    "    print(f\"  Final response: {final[:300]}\")\n",
    "    errors = trace.error_spans\n",
    "    print(f\"  Error spans: {len(errors)}\")\n",
    "    for es in errors:\n",
    "        print(f\"    - {es.event_type}: {es.error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List traces with filtering\n",
    "all_traces = client.list_traces(\n",
    "    TraceFilter(session_ids=session_ids)\n",
    ")\n",
    "print(f\"Listed {len(all_traces)} traces:\")\n",
    "for t in all_traces:\n",
    "    print(f\"  - {t.session_id}  spans={len(t.spans)}  \"\n",
    "          f\"tools={len(t.tool_calls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 3: Code-Based Evaluation\n",
    "\n",
    "The `CodeEvaluator` runs deterministic metrics over session aggregates — no LLM needed. Pre-built evaluators cover latency, turn count, error rate, token efficiency, and cost. You can also define custom metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import CodeEvaluator\n",
    "\n",
    "trace_filter = TraceFilter(session_ids=session_ids)\n",
    "\n",
    "presets = [\n",
    "    (\"latency\", CodeEvaluator.latency(threshold_ms=30000)),\n",
    "    (\"turn_count\", CodeEvaluator.turn_count(max_turns=10)),\n",
    "    (\"error_rate\", CodeEvaluator.error_rate(max_error_rate=0.1)),\n",
    "    (\"token_efficiency\", CodeEvaluator.token_efficiency(max_tokens=100000)),\n",
    "    (\"cost_per_session\", CodeEvaluator.cost_per_session(max_cost_usd=1.0)),\n",
    "]\n",
    "\n",
    "for name, evaluator in presets:\n",
    "    try:\n",
    "        report = asyncio.get_event_loop().run_until_complete(\n",
    "            asyncio.to_thread(\n",
    "                client.evaluate,\n",
    "                evaluator=evaluator,\n",
    "                filters=trace_filter,\n",
    "            )\n",
    "        )\n",
    "        print(f\"\\n[{name}]\")\n",
    "        print(report.summary())\n",
    "    except Exception as exc:\n",
    "        print(f\"\\n[{name}] Failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom metric: response length scoring\ndef response_length_score(session_summary: dict) -> float:\n    \"\"\"Score based on response token count — longer is better up to a point.\"\"\"\n    tokens = session_summary.get(\"output_tokens\") or 0\n    # Ideal range: 200-2000 tokens\n    if 200 <= tokens <= 2000:\n        return 1.0\n    elif tokens < 200:\n        return tokens / 200.0\n    else:\n        return max(0.0, 1.0 - (tokens - 2000) / 5000.0)\n\n\ncustom_eval = (\n    CodeEvaluator(\"custom_metrics\")\n    .add_metric(\"response_length\", response_length_score, threshold=0.5)\n)\n\ntry:\n    report = asyncio.get_event_loop().run_until_complete(\n        asyncio.to_thread(\n            client.evaluate,\n            evaluator=custom_eval,\n            filters=trace_filter,\n        )\n    )\n    print(\"[custom: response_length]\")\n    print(report.summary())\nexcept Exception as exc:\n    print(f\"Custom evaluator failed: {exc}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 4: LLM-as-Judge Evaluation\n",
    "\n",
    "Semantic evaluation using an LLM to judge agent quality. The SDK supports a 3-tier fallback: BigQuery `AI.GENERATE` → `ML.GENERATE_TEXT` → Gemini API. Pre-built judges evaluate **correctness**, **hallucination** (faithfulness), and **sentiment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import LLMAsJudge\n",
    "\n",
    "# Correctness evaluation\n",
    "judge_correctness = LLMAsJudge.correctness(threshold=0.6)\n",
    "try:\n",
    "    report = asyncio.get_event_loop().run_until_complete(\n",
    "        asyncio.to_thread(\n",
    "            client.evaluate,\n",
    "            evaluator=judge_correctness,\n",
    "            filters=trace_filter,\n",
    "        )\n",
    "    )\n",
    "    print(\"[LLM Judge: Correctness]\")\n",
    "    print(report.summary())\n",
    "    print(\"\\nPer-session details:\")\n",
    "    for ss in report.session_scores:\n",
    "        print(f\"  {ss.session_id}: scores={ss.scores} \"\n",
    "              f\"passed={ss.passed}\")\n",
    "        if ss.llm_feedback:\n",
    "            print(f\"    Feedback: {ss.llm_feedback[:200]}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Correctness judge failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hallucination (faithfulness) evaluation\n",
    "judge_hallucination = LLMAsJudge.hallucination()\n",
    "try:\n",
    "    report = asyncio.get_event_loop().run_until_complete(\n",
    "        asyncio.to_thread(\n",
    "            client.evaluate,\n",
    "            evaluator=judge_hallucination,\n",
    "            filters=trace_filter,\n",
    "        )\n",
    "    )\n",
    "    print(\"[LLM Judge: Hallucination/Faithfulness]\")\n",
    "    print(report.summary())\n",
    "except Exception as exc:\n",
    "    print(f\"Hallucination judge failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment evaluation\n",
    "judge_sentiment = LLMAsJudge.sentiment()\n",
    "try:\n",
    "    report = asyncio.get_event_loop().run_until_complete(\n",
    "        asyncio.to_thread(\n",
    "            client.evaluate,\n",
    "            evaluator=judge_sentiment,\n",
    "            filters=trace_filter,\n",
    "        )\n",
    "    )\n",
    "    print(\"[LLM Judge: Sentiment]\")\n",
    "    print(report.summary())\n",
    "except Exception as exc:\n",
    "    print(f\"Sentiment judge failed: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 5: Trajectory Matching\n",
    "\n",
    "Compare actual agent tool-call sequences against **golden trajectories**. Three match types:\n",
    "\n",
    "| `MatchType` | Description |\n",
    "|---|---|\n",
    "| `EXACT` | Tool calls must match exactly (order & count) |\n",
    "| `IN_ORDER` | Expected tools appear in order, extra tools allowed between |\n",
    "| `ANY_ORDER` | All expected tools present, any order |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import BigQueryTraceEvaluator\n",
    "from bigquery_agent_analytics.trace_evaluator import MatchType\n",
    "\n",
    "trace_evaluator = BigQueryTraceEvaluator(\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_id=DATASET_ID,\n",
    "    table_id=TABLE_ID,\n",
    ")\n",
    "print(\"BigQueryTraceEvaluator ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Tokyo trip (session index 1) — IN_ORDER match\n",
    "# The complex trip should call all 4 tools in sequence.\n",
    "golden_tokyo = [\n",
    "    {\"tool_name\": \"search_flights\"},\n",
    "    {\"tool_name\": \"search_hotels\"},\n",
    "    {\"tool_name\": \"get_weather_forecast\"},\n",
    "    {\"tool_name\": \"calculate_trip_budget\"},\n",
    "]\n",
    "\n",
    "try:\n",
    "    result = asyncio.get_event_loop().run_until_complete(\n",
    "        trace_evaluator.evaluate_session(\n",
    "            session_id=session_ids[1],\n",
    "            golden_trajectory=golden_tokyo,\n",
    "            match_type=MatchType.IN_ORDER,\n",
    "        )\n",
    "    )\n",
    "    print(\"[Trajectory: Tokyo trip — IN_ORDER]\")\n",
    "    print(f\"  Session : {result.session_id}\")\n",
    "    print(f\"  Status  : {result.eval_status}\")\n",
    "    print(f\"  Scores  : {result.scores}\")\n",
    "    if result.details:\n",
    "        print(f\"  Details : {json.dumps(result.details, indent=2)}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Trajectory evaluation failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare match types: EXACT vs ANY_ORDER on the same session\n",
    "for match_type in [MatchType.EXACT, MatchType.ANY_ORDER]:\n",
    "    try:\n",
    "        result = asyncio.get_event_loop().run_until_complete(\n",
    "            trace_evaluator.evaluate_session(\n",
    "                session_id=session_ids[1],\n",
    "                golden_trajectory=golden_tokyo,\n",
    "                match_type=match_type,\n",
    "            )\n",
    "        )\n",
    "        print(f\"\\n[Trajectory: Tokyo — {match_type.value}]\")\n",
    "        print(f\"  Status: {result.eval_status}  \"\n",
    "              f\"Scores: {result.scores}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"  {match_type.value} failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluation across all sessions\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"session_id\": session_ids[0],\n",
    "        \"expected_trajectory\": [\n",
    "            {\"tool_name\": \"search_flights\"},\n",
    "            {\"tool_name\": \"search_hotels\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"session_id\": session_ids[1],\n",
    "        \"expected_trajectory\": golden_tokyo,\n",
    "    },\n",
    "    {\n",
    "        \"session_id\": session_ids[2],\n",
    "        \"expected_trajectory\": [\n",
    "            {\"tool_name\": \"get_weather_forecast\"},\n",
    "            {\"tool_name\": \"search_flights\"},\n",
    "            {\"tool_name\": \"search_hotels\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "try:\n",
    "    batch_results = asyncio.get_event_loop().run_until_complete(\n",
    "        trace_evaluator.evaluate_batch(\n",
    "            eval_dataset=eval_dataset,\n",
    "            match_type=MatchType.IN_ORDER,\n",
    "        )\n",
    "    )\n",
    "    print(\"[Batch Trajectory Evaluation — IN_ORDER]\")\n",
    "    for r in batch_results:\n",
    "        print(f\"  {r.session_id}: {r.eval_status}  \"\n",
    "              f\"scores={r.scores}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Batch evaluation failed: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 6: Grader Pipeline\n",
    "\n",
    "Compose multiple evaluators (code + LLM) into a single **GraderPipeline** with configurable voting strategies: `WeightedStrategy`, `BinaryStrategy`, or `MajorityStrategy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import (\n",
    "    GraderPipeline,\n",
    "    WeightedStrategy,\n",
    "    BinaryStrategy,\n",
    "    MajorityStrategy,\n",
    ")\n",
    "\n",
    "# Build a weighted pipeline with code + LLM graders\n",
    "pipeline = (\n",
    "    GraderPipeline(WeightedStrategy(threshold=0.6))\n",
    "    .add_code_grader(\n",
    "        CodeEvaluator.latency(threshold_ms=30000),\n",
    "        weight=1.0,\n",
    "    )\n",
    "    .add_code_grader(\n",
    "        CodeEvaluator.error_rate(max_error_rate=0.1),\n",
    "        weight=1.0,\n",
    "    )\n",
    "    .add_llm_grader(\n",
    "        LLMAsJudge.correctness(threshold=0.6),\n",
    "        weight=2.0,\n",
    "    )\n",
    ")\n",
    "print(\"GraderPipeline built (weighted: code=1.0 + code=1.0 + llm=2.0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Construct session_summary from trace metadata and evaluate\n# We use the Tokyo trip trace (index 1) as an example.\nimport io, contextlib\n\ntrace_idx = 1\nif traces[trace_idx] is not None:\n    trace = traces[trace_idx]\n    session_summary = {\n        \"session_id\": trace.session_id,\n        \"total_events\": len(trace.spans),\n        \"tool_calls\": len(trace.tool_calls),\n        \"tool_errors\": len(trace.error_spans),\n        \"llm_calls\": sum(\n            1 for s in trace.spans\n            if s.event_type in (\"llm_request\", \"llm_response\")\n        ),\n        \"avg_latency_ms\": (\n            trace.total_latency_ms / max(len(trace.spans), 1)\n            if trace.total_latency_ms\n            else 0.0\n        ),\n        \"max_latency_ms\": max(\n            (s.latency_ms or 0 for s in trace.spans), default=0\n        ),\n        \"total_latency_ms\": trace.total_latency_ms or 0.0,\n        \"turn_count\": sum(\n            1 for s in trace.spans if s.event_type == \"user_message\"\n        ),\n        \"has_error\": len(trace.error_spans) > 0,\n        \"input_tokens\": sum(\n            s.attributes.get(\"input_tokens\", 0) or 0\n            for s in trace.spans\n        ),\n        \"output_tokens\": sum(\n            s.attributes.get(\"output_tokens\", 0) or 0\n            for s in trace.spans\n        ),\n        \"total_tokens\": sum(\n            s.attributes.get(\"total_tokens\", 0) or 0\n            for s in trace.spans\n        ),\n    }\n\n    # Get trace text (suppress render's print) and final response\n    buf = io.StringIO()\n    with contextlib.redirect_stdout(buf):\n        trace_text = trace.render(format=\"tree\")\n    if not isinstance(trace_text, str):\n        trace_text = buf.getvalue()\n    final_response = trace.final_response or \"\"\n\n    verdict = asyncio.get_event_loop().run_until_complete(\n        pipeline.evaluate(\n            session_summary=session_summary,\n            trace_text=trace_text,\n            final_response=final_response,\n        )\n    )\n\n    print(f\"[GraderPipeline — Weighted]\")\n    print(f\"  Final score : {verdict.final_score:.3f}\")\n    print(f\"  Passed      : {verdict.passed}\")\n    print(f\"  Strategy    : {verdict.strategy_name}\")\n    print(f\"  Grader breakdown:\")\n    for gr in verdict.grader_results:\n        print(f\"    - {gr.grader_name}: scores={gr.scores} \"\n              f\"passed={gr.passed}\")\nelse:\n    print(\"Trace not available — skipping pipeline evaluation.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo alternative strategies: Binary and Majority\nif traces[trace_idx] is not None:\n    for strategy_cls, strategy_name in [\n        (BinaryStrategy, \"Binary (all must pass)\"),\n        (MajorityStrategy, \"Majority\"),\n    ]:\n        alt_pipeline = (\n            GraderPipeline(strategy_cls())\n            .add_code_grader(\n                CodeEvaluator.latency(threshold_ms=30000),\n            )\n            .add_code_grader(\n                CodeEvaluator.error_rate(max_error_rate=0.1),\n            )\n            .add_llm_grader(\n                LLMAsJudge.correctness(threshold=0.6),\n            )\n        )\n        v = asyncio.get_event_loop().run_until_complete(\n            alt_pipeline.evaluate(\n                session_summary=session_summary,\n                trace_text=trace_text,\n                final_response=final_response,\n            )\n        )\n        print(f\"\\n[GraderPipeline — {strategy_name}]\")\n        print(f\"  Final score: {v.final_score:.3f}  \"\n              f\"Passed: {v.passed}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 7: Eval Suite & Validator\n",
    "\n",
    "The **EvalSuite** manages evaluation task definitions, supports capability-to-regression graduation, and exports to eval datasets. The **EvalValidator** performs sanity checks (ambiguity, balance, threshold consistency, duplicates, saturation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import (\n",
    "    EvalSuite,\n",
    "    EvalTaskDef,\n",
    "    EvalCategory,\n",
    "    EvalValidator,\n",
    ")\n",
    "\n",
    "suite = EvalSuite(name=\"travel_agent_evals\")\n",
    "\n",
    "# Add tasks from Phase 1 sessions\n",
    "suite.add_task(EvalTaskDef(\n",
    "    task_id=\"simple_trip_sf_ny\",\n",
    "    session_id=session_ids[0],\n",
    "    description=\"Simple SF->NY weekend trip — should call flights + hotels.\",\n",
    "    category=EvalCategory.CAPABILITY,\n",
    "    expected_trajectory=[\n",
    "        {\"tool_name\": \"search_flights\"},\n",
    "        {\"tool_name\": \"search_hotels\"},\n",
    "    ],\n",
    "    thresholds={\"trajectory_match\": 0.8, \"latency\": 0.7},\n",
    "    tags=[\"simple\", \"domestic\"],\n",
    "))\n",
    "\n",
    "suite.add_task(EvalTaskDef(\n",
    "    task_id=\"complex_trip_tokyo\",\n",
    "    session_id=session_ids[1],\n",
    "    description=\"Complex LA->Tokyo 5-day trip — all 4 tools expected.\",\n",
    "    category=EvalCategory.CAPABILITY,\n",
    "    expected_trajectory=golden_tokyo,\n",
    "    thresholds={\"trajectory_match\": 0.9, \"latency\": 0.6},\n",
    "    tags=[\"complex\", \"international\"],\n",
    "))\n",
    "\n",
    "suite.add_task(EvalTaskDef(\n",
    "    task_id=\"multiturn_paris\",\n",
    "    session_id=session_ids[2],\n",
    "    description=\"Multi-turn Chicago->Paris — weather, flights, hotels across 3 turns.\",\n",
    "    category=EvalCategory.REGRESSION,\n",
    "    expected_trajectory=[\n",
    "        {\"tool_name\": \"get_weather_forecast\"},\n",
    "        {\"tool_name\": \"search_flights\"},\n",
    "        {\"tool_name\": \"search_hotels\"},\n",
    "    ],\n",
    "    thresholds={\"trajectory_match\": 0.8},\n",
    "    tags=[\"multi-turn\", \"international\"],\n",
    "))\n",
    "\n",
    "print(f\"EvalSuite '{suite.name}' — {len(suite.get_tasks())} tasks added.\")\n",
    "for t in suite.get_tasks():\n",
    "    print(f\"  [{t.category.value}] {t.task_id}: {t.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suite health check\n",
    "pass_history = {\n",
    "    \"simple_trip_sf_ny\": [True, True, True, True, True],\n",
    "    \"complex_trip_tokyo\": [True, False, True, True, True],\n",
    "    \"multiturn_paris\": [True, True, True, True, True],\n",
    "}\n",
    "\n",
    "health = suite.check_health(pass_history=pass_history)\n",
    "print(\"[Suite Health]\")\n",
    "print(f\"  Total tasks      : {health.total_tasks}\")\n",
    "print(f\"  Capability tasks : {health.capability_tasks}\")\n",
    "print(f\"  Regression tasks : {health.regression_tasks}\")\n",
    "print(f\"  Positive cases   : {health.positive_cases}\")\n",
    "print(f\"  Negative cases   : {health.negative_cases}\")\n",
    "print(f\"  Balance ratio    : {health.balance_ratio:.2f}\")\n",
    "print(f\"  Saturated tasks  : {health.saturated_task_ids}\")\n",
    "if health.warnings:\n",
    "    print(f\"  Warnings:\")\n",
    "    for w in health.warnings:\n",
    "        print(f\"    - {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate suite\n",
    "warnings = EvalValidator.validate_suite(\n",
    "    suite, pass_history=pass_history\n",
    ")\n",
    "print(f\"[EvalValidator] {len(warnings)} warnings:\")\n",
    "for w in warnings:\n",
    "    print(f\"  [{w.severity}] {w.check_name} \"\n",
    "          f\"(task={w.task_id}): {w.message}\")\n",
    "if not warnings:\n",
    "    print(\"  No warnings — suite looks healthy!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to eval dataset\n",
    "eval_ds = suite.to_eval_dataset()\n",
    "print(f\"Exported {len(eval_ds)} tasks to eval dataset format:\")\n",
    "for entry in eval_ds:\n",
    "    print(f\"  session_id={entry['session_id']}  \"\n",
    "          f\"trajectory_len=\"\n",
    "          f\"{len(entry.get('expected_trajectory', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 8: Multi-Trial Evaluation\n",
    "\n",
    "LLM agents are non-deterministic. The **TrialRunner** repeats evaluation N times to compute:\n",
    "- **pass@k** — probability that at least one trial passes\n",
    "- **pass^k** — probability that all k trials pass\n",
    "- **per_trial_pass_rate** — fraction of trials that passed\n",
    "- **mean_scores** and **score_std_dev** — statistics across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import TrialRunner\n",
    "\n",
    "trial_runner = TrialRunner(\n",
    "    evaluator=trace_evaluator,\n",
    "    num_trials=3,\n",
    "    concurrency=3,\n",
    ")\n",
    "\n",
    "try:\n",
    "    trial_report = asyncio.get_event_loop().run_until_complete(\n",
    "        trial_runner.run_trials(\n",
    "            session_id=session_ids[1],\n",
    "            golden_trajectory=golden_tokyo,\n",
    "            match_type=MatchType.IN_ORDER,\n",
    "            use_llm_judge=True,\n",
    "        )\n",
    "    )\n",
    "    print(\"[Multi-Trial Report — Tokyo trip, 3 trials]\")\n",
    "    print(f\"  pass@k             : {trial_report.pass_at_k:.3f}\")\n",
    "    print(f\"  pass^k             : {trial_report.pass_pow_k:.3f}\")\n",
    "    print(f\"  per_trial_pass_rate: {trial_report.per_trial_pass_rate:.3f}\")\n",
    "    print(f\"  mean_scores        : {trial_report.mean_scores}\")\n",
    "    print(f\"  score_std_dev      : {trial_report.score_std_dev}\")\n",
    "    print(f\"\\n  Per-trial results:\")\n",
    "    for tr in trial_report.trial_results:\n",
    "        print(f\"    Trial {tr.trial_index}: passed={tr.passed} \"\n",
    "              f\"scores={tr.scores}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Multi-trial evaluation failed: {exc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 9: Insights Report\n",
    "\n",
    "The **Insights** pipeline is a multi-stage AI-powered analysis:\n",
    "1. Session filtering and metadata extraction\n",
    "2. Per-session facet extraction (goals, outcomes, friction)\n",
    "3. Cross-session aggregation\n",
    "4. Multi-prompt analysis (7 specialised prompts)\n",
    "5. Executive summary generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import InsightsConfig\n",
    "\n",
    "try:\n",
    "    insights_report = asyncio.get_event_loop().run_until_complete(\n",
    "        asyncio.to_thread(\n",
    "            client.insights,\n",
    "            filters=TraceFilter(session_ids=session_ids),\n",
    "            config=InsightsConfig(\n",
    "                max_sessions=10,\n",
    "                min_events_per_session=3,\n",
    "                min_turns_per_session=1,\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(\"[Insights Report]\")\n",
    "    print(insights_report.summary())\n",
    "except Exception as exc:\n",
    "    print(f\"Insights generation failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executive summary\n",
    "try:\n",
    "    if insights_report.executive_summary:\n",
    "        print(\"[Executive Summary]\")\n",
    "        print(insights_report.executive_summary)\n",
    "    else:\n",
    "        print(\"No executive summary generated.\")\n",
    "except NameError:\n",
    "    print(\"Insights report not available — run previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis sections\n",
    "try:\n",
    "    for section in insights_report.analysis_sections:\n",
    "        print(f\"\\n## {section.title}\")\n",
    "        print(section.content[:2000])\n",
    "        if len(section.content) > 2000:\n",
    "            print(\"  ... (truncated)\")\n",
    "except NameError:\n",
    "    print(\"Insights report not available — run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Per-session facets\ntry:\n    print(\"[Session Facets]\")\n    for facet in insights_report.session_facets:\n        print(f\"\\n  Session: {facet.session_id}\")\n        if facet.goal_categories:\n            print(f\"    Goal categories  : {facet.goal_categories}\")\n        if facet.outcome:\n            print(f\"    Outcome          : {facet.outcome}\")\n        if facet.satisfaction:\n            print(f\"    Satisfaction     : {facet.satisfaction}\")\n        if facet.key_topics:\n            print(f\"    Key topics       : {facet.key_topics}\")\n        print(f\"    Effectiveness    : {facet.agent_effectiveness}\")\n        print(f\"    Primary success  : {facet.primary_success}\")\nexcept NameError:\n    print(\"Insights report not available — run previous cells first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 10: Deep Analysis & Drift Detection\n",
    "\n",
    "**Deep analysis** performs question distribution analysis — grouping user queries into semantic categories. **Drift detection** compares production questions against a golden dataset to measure coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigquery_agent_analytics import AnalysisConfig\n",
    "\n",
    "try:\n",
    "    question_dist = asyncio.get_event_loop().run_until_complete(\n",
    "        asyncio.to_thread(\n",
    "            client.deep_analysis,\n",
    "            filters=TraceFilter(session_ids=session_ids),\n",
    "            configuration=AnalysisConfig(\n",
    "                mode=\"frequently_asked\",\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    print(\"[Deep Analysis — Frequently Asked Questions]\")\n",
    "    print(question_dist.summary())\n",
    "    print(f\"\\nTotal questions: {question_dist.total_questions}\")\n",
    "    for cat in question_dist.categories:\n",
    "        print(f\"\\n  Category: {cat.name} \"\n",
    "              f\"(count={cat.count}, {cat.percentage:.1f}%)\")\n",
    "        for ex in cat.examples[:3]:\n",
    "            print(f\"    - {ex}\")\n",
    "except Exception as exc:\n",
    "    print(f\"Deep analysis failed: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drift detection requires a golden dataset table in BigQuery.\n",
    "# Below shows the API pattern — uncomment and provide your golden table.\n",
    "\n",
    "# from bigquery_agent_analytics import DriftReport\n",
    "#\n",
    "# drift_report = asyncio.get_event_loop().run_until_complete(\n",
    "#     asyncio.to_thread(\n",
    "#         client.drift_detection,\n",
    "#         golden_dataset=\"your_project.your_dataset.golden_questions\",\n",
    "#         filters=TraceFilter(session_ids=session_ids),\n",
    "#     )\n",
    "# )\n",
    "# print(\"[Drift Detection]\")\n",
    "# print(drift_report.summary())\n",
    "# print(f\"  Coverage: {drift_report.coverage_percentage:.1f}%\")\n",
    "# print(f\"  Uncovered questions: {drift_report.uncovered_questions}\")\n",
    "# print(f\"  New questions: {drift_report.new_questions}\")\n",
    "\n",
    "print(\"Drift detection requires a golden dataset table — see commented code above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated the full **BigQuery Agent Analytics SDK** lifecycle:\n",
    "\n",
    "| Phase | Feature | Description |\n",
    "|---|---|---|\n",
    "| 1 | **Agent Execution** | Ran a travel planner agent with 4 tools, logged traces to BigQuery |\n",
    "| 2 | **Trace Retrieval** | Fetched traces, rendered hierarchical DAGs, inspected tool calls |\n",
    "| 3 | **Code Evaluation** | Latency, turn count, error rate, token efficiency, cost, custom metrics |\n",
    "| 4 | **LLM-as-Judge** | Correctness, hallucination/faithfulness, sentiment scoring |\n",
    "| 5 | **Trajectory Matching** | EXACT, IN_ORDER, ANY_ORDER matching against golden trajectories |\n",
    "| 6 | **Grader Pipeline** | Composed evaluators with Weighted, Binary, and Majority strategies |\n",
    "| 7 | **Eval Suite** | Task management, health checks, validation, dataset export |\n",
    "| 8 | **Multi-Trial** | N-trial evaluation with pass@k, pass^k, and score statistics |\n",
    "| 9 | **Insights** | AI-powered multi-stage analysis with executive summaries |\n",
    "| 10 | **Deep Analysis** | Question distribution analysis and drift detection patterns |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Production-ready logging**: The `BigQueryAgentAnalyticsPlugin` integrates directly with ADK's Runner to capture every agent event.\n",
    "- **Multi-level evaluation**: From deterministic code metrics to semantic LLM judges to trajectory matching — evaluate agents at every level.\n",
    "- **Composable grading**: The `GraderPipeline` lets you combine evaluators with flexible voting strategies for nuanced pass/fail decisions.\n",
    "- **Suite management**: `EvalSuite` + `EvalValidator` support capability-to-regression graduation and health monitoring.\n",
    "- **Non-determinism handling**: `TrialRunner` repeats evaluations to compute robust pass@k/pass^k metrics.\n",
    "- **AI-powered insights**: The insights pipeline and deep analysis provide actionable intelligence about agent behavior at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "try:\n",
    "    asyncio.get_event_loop().run_until_complete(\n",
    "        plugin.shutdown(timeout=10.0)\n",
    "    )\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(\"\\nDemo complete!\")\n",
    "print(f\"Sessions: {session_ids}\")\n",
    "print(f\"Traces logged to: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 0,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
